# CSV to Data Warehouse ETL Pipeline using Airflow

This project demonstrates a simple yet production-style **ETL pipeline** that:
-  **Extracts** data from a CSV file
-  **Transforms** it using Python (Pandas)
-  **Loads** it into a PostgreSQL data warehouse
-   Uses **Apache Airflow** for task orchestration and scheduling



## Tech Stack

| Component       | Tech                                |
|----------------|-------------------------------------|
| Language        | Python 3.x                          |
| Workflow Engine | Apache Airflow                     |
| Database        | PostgreSQL                         |
| Data Processing | Pandas                              |
| Containerization| Docker + Docker Compose            |
| Scheduling      | Airflow DAGs (daily trigger)       |

## Contributing

We welcome contributions from everyone!

1. **Fork** this repository.
2. **Create** a new branch:
   ```bash
   git checkout -b feature/your-feature-name
   ```
3. **Commit** your changes:
   ```bash
   git commit -m "Add your message"
   ```
4. **Push** the changes:
   ```bash
   git push origin feature/your-feature-name
   ```
5. **Open** a Pull Request.



## License  
This project is licensed under the [MIT License](LICENSE).



## Author
 [**Aakaash M S**](https://github.com/msaakaash)





