# ğŸ“Š CSV to Data Warehouse ETL Pipeline using Airflow

This project demonstrates a simple yet production-style **ETL pipeline** that:
- ğŸ“¥ **Extracts** data from a CSV file
- ğŸ”„ **Transforms** it using Python (Pandas)
- ğŸ›¢ï¸ **Loads** it into a PostgreSQL data warehouse
- â° Uses **Apache Airflow** for task orchestration and scheduling



## ğŸš€ Tech Stack

| Component       | Tech                                |
|----------------|-------------------------------------|
| Language        | Python 3.x                          |
| Workflow Engine | Apache Airflow                     |
| Database        | PostgreSQL                         |
| Data Processing | Pandas                              |
| Containerization| Docker + Docker Compose            |
| Scheduling      | Airflow DAGs (daily trigger)       |



## ğŸ“ Project Structure

```plaintext
csv-datawarehouse-etl-airflow/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ etl_pipeline.py         # Airflow DAG
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sales_data.csv          # Sample input data
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract.py              # Extract from CSV
â”‚   â”œâ”€â”€ transform.py            # Clean & enhance data
â”‚   â””â”€â”€ load.py                 # Load into PostgreSQL
â”œâ”€â”€ docker-compose.yml          # Docker for Airflow + Postgres
â”œâ”€â”€ requirements.txt            # Python libraries
â””â”€â”€ README.md

```

## Author
 [**Aakaash M S**](https://github.com/msaakaash)





